# -*- coding: utf-8 -*-
"""110263008_Lab8_Homework.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/170n8dsPlIaazZkQMvpqHYaO4lodKEU6q
"""

!pip -q install torchinfo

from __future__ import annotations
import random, shutil, time, zipfile, os
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Tuple, Dict, List

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as T
from torchinfo import summary
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas

try:
    from tqdm.auto import tqdm  # 更美觀的進度條
except ImportError:            # 使用者環境若沒安裝 tqdm 就略過
    tqdm = lambda x, **_: x

# ---------------------------- Config -------------------------
@dataclass
class Config:
    # 資料相關
    zip_path:   Path = Path("/content/dataset.zip")  # ← 修改為你的資料集 zip
    extract_to: Path = Path("./dataset")
    split_root: Path = Path("./dataset_split")
    train_ratio: float = 0.7

    # 訓練超參數
    batch_size: int = 64
    lr: float = 3e-4
    epochs: int = 30
    seed: int = 42

    # ViT 參數
    img_size: int = 28
    patch: int = 4
    dim: int = 64
    depth: int = 6
    heads: int = 4
    mlp_dim: int = 128

    # 輸出
    student_id: str = "110263008"
    save_dir: Path = Path("/content/outputs")

cfg = Config()                    # 全域唯一設定物件
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----------------------- Utility -----------------------------
def set_seed(seed: int) -> None:
    """Fix random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def unzip_dataset(cfg: Config) -> None:
    """Unzip dataset only once."""
    if cfg.extract_to.exists():
        print("✅ Dataset folder already exists.")
        return
    with zipfile.ZipFile(cfg.zip_path, "r") as zf:
        zf.extractall(cfg.extract_to)
    print("✅ Unzipped dataset!")

def train_test_split(cfg: Config) -> None:
    """Split original dataset into train / test folders (7:3)."""
    if cfg.split_root.exists():
        print("✅ Split dataset already exists.")
        return

    for subset in ("train", "test"):
        for cls in cfg.extract_to.iterdir():
            (cfg.split_root / subset / cls.name).mkdir(parents=True, exist_ok=True)

    for cls_dir in cfg.extract_to.iterdir():
        images = sorted(cls_dir.iterdir())
        random.shuffle(images)
        cut = int(len(images) * cfg.train_ratio)
        for idx, img_path in enumerate(images):
            subset = "train" if idx < cut else "test"
            dst = cfg.split_root / subset / cls_dir.name / img_path.name
            shutil.copyfile(img_path, dst)

    print("✅ Dataset split complete!")

def build_dataloaders(cfg: Config) -> Tuple[DataLoader, DataLoader, List[str]]:
    """Return train / test dataloaders."""
    transform = T.Compose([
        T.Resize((cfg.img_size, cfg.img_size)),
        T.Grayscale(),
        T.ToTensor(),
        T.Normalize((0.5,), (0.5,))
    ])
    train_set = torchvision.datasets.ImageFolder(cfg.split_root / "train", transform)
    test_set  = torchvision.datasets.ImageFolder(cfg.split_root / "test",  transform)

    train_loader = DataLoader(
        train_set, batch_size=cfg.batch_size, shuffle=True,
        num_workers=2, pin_memory=True
    )
    test_loader = DataLoader(
        test_set, batch_size=cfg.batch_size, shuffle=False,
        num_workers=2, pin_memory=True
    )
    print(f"Classes: {train_set.classes} | Train: {len(train_set)} | Test: {len(test_set)}")
    return train_loader, test_loader, train_set.classes

# -------------------  Vision Transformer --------------------
def pair(x):                                      # 與原始實作一致
    return x if isinstance(x, tuple) else (x, x)

class PreNorm(nn.Module):
    def __init__(self, dim: int, fn: nn.Module):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn   = fn

    def forward(self, x: torch.Tensor, **kw):     # type: ignore[override]
        return x + self.fn(self.norm(x), **kw)

class FeedForward(nn.Module):
    def __init__(self, dim: int, hidden: int, drop: float = 0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden),
            nn.GELU(),
            nn.Dropout(drop),
            nn.Linear(hidden, dim),
            nn.Dropout(drop)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        return self.net(x)

class Attention(nn.Module):
    def __init__(self, dim: int, heads: int = 8,
                 dim_head: int = 64, drop: float = 0.):
        super().__init__()
        self.heads = heads
        inner_dim  = dim_head * heads
        self.scale = dim_head ** -0.5

        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(drop))
        self.drop   = nn.Dropout(drop)

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        b, n, _ = x.shape
        q, k, v = self.to_qkv(x).chunk(3, dim=-1)
        q = q.view(b, n, self.heads, -1).transpose(1, 2)
        k = k.view(b, n, self.heads, -1).transpose(1, 2)
        v = v.view(b, n, self.heads, -1).transpose(1, 2)

        dots = (q @ k.transpose(-1, -2)) * self.scale
        attn = self.drop(dots.softmax(dim=-1))
        out  = (attn @ v).transpose(1, 2).contiguous().view(b, n, -1)
        return self.to_out(out)

class Transformer(nn.Module):
    def __init__(self, dim: int, depth: int, heads: int,
                 dim_head: int, mlp_dim: int, drop: float = 0.):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.ModuleList([
                PreNorm(dim, Attention(dim, heads, dim_head, drop)),
                PreNorm(dim, FeedForward(dim, mlp_dim, drop))
            ]) for _ in range(depth)
        ])

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        for attn, ff in self.layers:
            x = attn(x)
            x = ff(x)
        return x

class ViT(nn.Module):
    def __init__(
        self, *, image_size: int, patch_size: int, num_classes: int, dim: int,
        depth: int, heads: int, mlp_dim: int, pool: str = "cls",
        channels: int = 1, dim_head: int = 64, dropout: float = 0.,
        emb_dropout: float = 0.
    ):
        super().__init__()
        img_h, img_w = pair(image_size)
        p_h, p_w     = pair(patch_size)
        assert img_h % p_h == 0 and img_w % p_w == 0, "Image must be divisible by patch size"

        num_patches = (img_h // p_h) * (img_w // p_w)
        patch_dim   = channels * p_h * p_w

        self.to_patch   = nn.Unfold(kernel_size=(p_h, p_w), stride=(p_h, p_w))
        self.patch_proj = nn.Linear(patch_dim, dim)

        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        self.dropout   = nn.Dropout(emb_dropout)

        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)
        self.pool        = pool
        self.norm        = nn.LayerNorm(dim)
        self.mlp_head    = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, num_classes)
        )

    def forward(self, img: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        b = img.shape[0]
        x = self.to_patch(img).transpose(1, 2)             # (B, N, patch_dim)
        x = self.patch_proj(x)                             # (B, N, dim)

        cls_tok = self.cls_token.expand(b, -1, -1)         # (B, 1, dim)
        x = torch.cat((cls_tok, x), dim=1)                 # (B, N+1, dim)
        x = x + self.pos_embed[:, :x.size(1)]
        x = self.dropout(x)

        x = self.transformer(x)
        x = self.norm(x[:, 0] if self.pool == "cls" else x.mean(dim=1))
        return self.mlp_head(x)

# -------------------- Training / Evaluation -----------------
def train_one_epoch(model: nn.Module, loader: DataLoader,
                    optim_: optim.Optimizer, criterion: nn.Module) -> float:
    model.train()
    running_loss = 0.0
    for i, (x, y) in enumerate(tqdm(loader, desc="Train", leave=False)):
        x, y = x.to(device), y.to(device)
        optim_.zero_grad()
        out  = model(x)
        loss = criterion(out, y)
        loss.backward()
        optim_.step()
        running_loss += loss.item()

        if i % 100 == 0:
            print(f"  Batch {i:03d} | loss {loss.item():.4f}")

    return running_loss / len(loader)

@torch.no_grad()
def evaluate(model: nn.Module, loader: DataLoader,
             criterion: nn.Module) -> Tuple[float, float]:
    model.eval()
    total_loss, correct = 0.0, 0
    n_samples = len(loader.dataset)

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        logits = model(x)
        total_loss += criterion(logits, y).item()
        correct   += (logits.argmax(1) == y).sum().item()

    acc = 100 * correct / n_samples
    print(f"  ▶ Test loss {total_loss/len(loader):.4f} | acc {acc:.2f}%")
    return total_loss / len(loader), acc

# --------------------------- Plots --------------------------
def save_model_summary(model: nn.Module, cfg: Config) -> None:
    cfg.save_dir.mkdir(parents=True, exist_ok=True)
    summary_str = str(summary(model, input_size=(cfg.batch_size, 1, cfg.img_size, cfg.img_size)))
    fig = plt.figure(figsize=(10, max(6, len(summary_str.splitlines()) * 0.22)))
    FigureCanvas(fig)
    fig.text(0, 1, summary_str, va="top", family="monospace", fontsize=6)
    fig.tight_layout()
    out_path = cfg.save_dir / "a_summary.png"
    fig.savefig(out_path, dpi=300)
    plt.close(fig)
    print("🖼 (a) summary →", out_path)

def plot_history(train_loss: List[float], test_loss: List[float],
                 test_acc: List[float], cfg: Config) -> None:
    epochs_arr = np.arange(1, cfg.epochs + 1)
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_arr, train_loss, label="Train Loss")
    plt.plot(epochs_arr, test_loss,  label="Test Loss")
    plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Loss Curve"); plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(epochs_arr, test_acc,   label="Test Acc (%)")
    plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.title("Test Accuracy"); plt.legend()
    plt.tight_layout()
    out_path = cfg.save_dir / "b_history.png"
    plt.savefig(out_path, dpi=300)
    plt.close()
    print("🖼 (b) history →", out_path)

@torch.no_grad()
def save_confusion(model: nn.Module, loader: DataLoader, classes: List[str],
                   cfg: Config) -> None:
    y_true, y_pred = [], []
    model.eval()
    for x, y in loader:
        logits = model(x.to(device))
        y_true.extend(y.cpu().numpy())
        y_pred.extend(logits.argmax(1).cpu().numpy())
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=classes, yticklabels=classes)
    plt.xlabel("Pred"); plt.ylabel("True"); plt.title("Confusion Matrix")
    plt.tight_layout()
    out_path = cfg.save_dir / "c_confmat.png"
    plt.savefig(out_path, dpi=300)
    plt.close()
    print("🖼 (c) confusion →", out_path)

@torch.no_grad()
def save_examples(model: nn.Module, loader: DataLoader, classes: List[str],
                  k: int, cfg: Config) -> None:
    """Save k examples per class with predictions colored."""
    bucket: Dict[int, List[Tuple[torch.Tensor, bool, int]]] = {c: [] for c in range(len(classes))}
    model.eval()
    for x, y in loader:
        preds = model(x.to(device)).argmax(1).cpu()
        for img, true_label, pred in zip(x, y, preds):
            if len(bucket[true_label.item()]) < k:
                bucket[true_label.item()].append((img, pred.item() == true_label.item(), pred.item()))
        if all(len(v) == k for v in bucket.values()):
            break

    rows, cols = k, len(classes)
    plt.figure(figsize=(2.2 * cols, 2.2 * rows))
    for c in range(cols):
        for r in range(rows):
            img, ok, pred = bucket[c][r]
            ax = plt.subplot(rows, cols, r * cols + c + 1)
            ax.imshow(img.squeeze() * 0.5 + 0.5, cmap="gray")
            ax.set_title(f"T:{classes[c]}\nP:{classes[pred]}",
                         color="g" if ok else "r", fontsize=7)
            ax.axis("off")
    plt.suptitle("Test Predictions (green = ✓, red = ✗)")
    plt.tight_layout()
    out_path = cfg.save_dir / "d_examples.png"
    plt.savefig(out_path, dpi=300)
    plt.close()
    print("🖼 (d) examples →", out_path)

# ------------------------------ Main ------------------------
def main(cfg: Config) -> None:
    print("===== Config =====")
    print(asdict(cfg))
    set_seed(cfg.seed)

    # 1. 資料前處理
    unzip_dataset(cfg)
    train_test_split(cfg)
    train_loader, test_loader, classes = build_dataloaders(cfg)

    # 2. 建立模型
    model = ViT(
        image_size=cfg.img_size, patch_size=cfg.patch,
        num_classes=len(classes), dim=cfg.dim, depth=cfg.depth,
        heads=cfg.heads, mlp_dim=cfg.mlp_dim, channels=1
    ).to(device)
    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)
    criterion = nn.CrossEntropyLoss()

    # 3. 模型摘要（圖 a 需要）
    save_model_summary(model, cfg)

    # 4. 訓練
    train_hist, test_loss_hist, test_acc_hist = [], [], []
    start_time = time.time()
    for ep in range(1, cfg.epochs + 1):
        print(f"Epoch {ep}/{cfg.epochs}")
        tr_loss = train_one_epoch(model, train_loader, optimizer, criterion)
        te_loss, te_acc = evaluate(model, test_loader, criterion)

        train_hist.append(tr_loss)
        test_loss_hist.append(te_loss)
        test_acc_hist.append(te_acc)

    print(f"⏱ Total time {time.time() - start_time:.1f}s")

    # 5. 儲存模型權重
    weight_path = f"{cfg.student_id}_Lab8_Homework.pth"
    torch.save(model.state_dict(), weight_path)
    print("✅ Weights saved as", weight_path)

    # 6. 繪圖
    cfg.save_dir.mkdir(parents=True, exist_ok=True)
    plot_history(train_hist, test_loss_hist, test_acc_hist, cfg)
    save_confusion(model, test_loader, classes, cfg)
    save_examples(model, test_loader, classes, k=4, cfg=cfg)

    print("\n🎉 四張圖全部完成！檔案都在:", cfg.save_dir)

# --------------------- Script entry  ------------------------
if __name__ == "__main__":
    main(cfg)