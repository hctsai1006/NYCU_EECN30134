# -*- coding: utf-8 -*-
"""110263008_Lab8_Homework.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/170n8dsPlIaazZkQMvpqHYaO4lodKEU6q
"""

!pip -q install torchinfo

from __future__ import annotations
import random, shutil, time, zipfile, os
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Tuple, Dict, List

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as T
from torchinfo import summary
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas

try:
    from tqdm.auto import tqdm  # æ›´ç¾è§€çš„é€²åº¦æ¢
except ImportError:            # ä½¿ç”¨è€…ç’°å¢ƒè‹¥æ²’å®‰è£ tqdm å°±ç•¥é
    tqdm = lambda x, **_: x

# ---------------------------- Config -------------------------
@dataclass
class Config:
    # è³‡æ–™ç›¸é—œ
    zip_path:   Path = Path("/content/dataset.zip")  # â† ä¿®æ”¹ç‚ºä½ çš„è³‡æ–™é›† zip
    extract_to: Path = Path("./dataset")
    split_root: Path = Path("./dataset_split")
    train_ratio: float = 0.7

    # è¨“ç·´è¶…åƒæ•¸
    batch_size: int = 64
    lr: float = 3e-4
    epochs: int = 30
    seed: int = 42

    # ViT åƒæ•¸
    img_size: int = 28
    patch: int = 4
    dim: int = 64
    depth: int = 6
    heads: int = 4
    mlp_dim: int = 128

    # è¼¸å‡º
    student_id: str = "110263008"
    save_dir: Path = Path("/content/outputs")

cfg = Config()                    # å…¨åŸŸå”¯ä¸€è¨­å®šç‰©ä»¶
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----------------------- Utility -----------------------------
def set_seed(seed: int) -> None:
    """Fix random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def unzip_dataset(cfg: Config) -> None:
    """Unzip dataset only once."""
    if cfg.extract_to.exists():
        print("âœ… Dataset folder already exists.")
        return
    with zipfile.ZipFile(cfg.zip_path, "r") as zf:
        zf.extractall(cfg.extract_to)
    print("âœ… Unzipped dataset!")

def train_test_split(cfg: Config) -> None:
    """Split original dataset into train / test folders (7:3)."""
    if cfg.split_root.exists():
        print("âœ… Split dataset already exists.")
        return

    for subset in ("train", "test"):
        for cls in cfg.extract_to.iterdir():
            (cfg.split_root / subset / cls.name).mkdir(parents=True, exist_ok=True)

    for cls_dir in cfg.extract_to.iterdir():
        images = sorted(cls_dir.iterdir())
        random.shuffle(images)
        cut = int(len(images) * cfg.train_ratio)
        for idx, img_path in enumerate(images):
            subset = "train" if idx < cut else "test"
            dst = cfg.split_root / subset / cls_dir.name / img_path.name
            shutil.copyfile(img_path, dst)

    print("âœ… Dataset split complete!")

def build_dataloaders(cfg: Config) -> Tuple[DataLoader, DataLoader, List[str]]:
    """Return train / test dataloaders."""
    transform = T.Compose([
        T.Resize((cfg.img_size, cfg.img_size)),
        T.Grayscale(),
        T.ToTensor(),
        T.Normalize((0.5,), (0.5,))
    ])
    train_set = torchvision.datasets.ImageFolder(cfg.split_root / "train", transform)
    test_set  = torchvision.datasets.ImageFolder(cfg.split_root / "test",  transform)

    train_loader = DataLoader(
        train_set, batch_size=cfg.batch_size, shuffle=True,
        num_workers=2, pin_memory=True
    )
    test_loader = DataLoader(
        test_set, batch_size=cfg.batch_size, shuffle=False,
        num_workers=2, pin_memory=True
    )
    print(f"Classes: {train_set.classes} | Train: {len(train_set)} | Test: {len(test_set)}")
    return train_loader, test_loader, train_set.classes

# -------------------  Vision Transformer --------------------
def pair(x):                                      # èˆ‡åŸå§‹å¯¦ä½œä¸€è‡´
    return x if isinstance(x, tuple) else (x, x)

class PreNorm(nn.Module):
    def __init__(self, dim: int, fn: nn.Module):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn   = fn

    def forward(self, x: torch.Tensor, **kw):     # type: ignore[override]
        return x + self.fn(self.norm(x), **kw)

class FeedForward(nn.Module):
    def __init__(self, dim: int, hidden: int, drop: float = 0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden),
            nn.GELU(),
            nn.Dropout(drop),
            nn.Linear(hidden, dim),
            nn.Dropout(drop)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        return self.net(x)

class Attention(nn.Module):
    def __init__(self, dim: int, heads: int = 8,
                 dim_head: int = 64, drop: float = 0.):
        super().__init__()
        self.heads = heads
        inner_dim  = dim_head * heads
        self.scale = dim_head ** -0.5

        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(drop))
        self.drop   = nn.Dropout(drop)

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        b, n, _ = x.shape
        q, k, v = self.to_qkv(x).chunk(3, dim=-1)
        q = q.view(b, n, self.heads, -1).transpose(1, 2)
        k = k.view(b, n, self.heads, -1).transpose(1, 2)
        v = v.view(b, n, self.heads, -1).transpose(1, 2)

        dots = (q @ k.transpose(-1, -2)) * self.scale
        attn = self.drop(dots.softmax(dim=-1))
        out  = (attn @ v).transpose(1, 2).contiguous().view(b, n, -1)
        return self.to_out(out)

class Transformer(nn.Module):
    def __init__(self, dim: int, depth: int, heads: int,
                 dim_head: int, mlp_dim: int, drop: float = 0.):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.ModuleList([
                PreNorm(dim, Attention(dim, heads, dim_head, drop)),
                PreNorm(dim, FeedForward(dim, mlp_dim, drop))
            ]) for _ in range(depth)
        ])

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        for attn, ff in self.layers:
            x = attn(x)
            x = ff(x)
        return x

class ViT(nn.Module):
    def __init__(
        self, *, image_size: int, patch_size: int, num_classes: int, dim: int,
        depth: int, heads: int, mlp_dim: int, pool: str = "cls",
        channels: int = 1, dim_head: int = 64, dropout: float = 0.,
        emb_dropout: float = 0.
    ):
        super().__init__()
        img_h, img_w = pair(image_size)
        p_h, p_w     = pair(patch_size)
        assert img_h % p_h == 0 and img_w % p_w == 0, "Image must be divisible by patch size"

        num_patches = (img_h // p_h) * (img_w // p_w)
        patch_dim   = channels * p_h * p_w

        self.to_patch   = nn.Unfold(kernel_size=(p_h, p_w), stride=(p_h, p_w))
        self.patch_proj = nn.Linear(patch_dim, dim)

        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        self.dropout   = nn.Dropout(emb_dropout)

        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)
        self.pool        = pool
        self.norm        = nn.LayerNorm(dim)
        self.mlp_head    = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, num_classes)
        )

    def forward(self, img: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        b = img.shape[0]
        x = self.to_patch(img).transpose(1, 2)             # (B, N, patch_dim)
        x = self.patch_proj(x)                             # (B, N, dim)

        cls_tok = self.cls_token.expand(b, -1, -1)         # (B, 1, dim)
        x = torch.cat((cls_tok, x), dim=1)                 # (B, N+1, dim)
        x = x + self.pos_embed[:, :x.size(1)]
        x = self.dropout(x)

        x = self.transformer(x)
        x = self.norm(x[:, 0] if self.pool == "cls" else x.mean(dim=1))
        return self.mlp_head(x)

# -------------------- Training / Evaluation -----------------
def train_one_epoch(model: nn.Module, loader: DataLoader,
                    optim_: optim.Optimizer, criterion: nn.Module) -> float:
    model.train()
    running_loss = 0.0
    for i, (x, y) in enumerate(tqdm(loader, desc="Train", leave=False)):
        x, y = x.to(device), y.to(device)
        optim_.zero_grad()
        out  = model(x)
        loss = criterion(out, y)
        loss.backward()
        optim_.step()
        running_loss += loss.item()

        if i % 100 == 0:
            print(f"  Batch {i:03d} | loss {loss.item():.4f}")

    return running_loss / len(loader)

@torch.no_grad()
def evaluate(model: nn.Module, loader: DataLoader,
             criterion: nn.Module) -> Tuple[float, float]:
    model.eval()
    total_loss, correct = 0.0, 0
    n_samples = len(loader.dataset)

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        logits = model(x)
        total_loss += criterion(logits, y).item()
        correct   += (logits.argmax(1) == y).sum().item()

    acc = 100 * correct / n_samples
    print(f"  â–¶ Test loss {total_loss/len(loader):.4f} | acc {acc:.2f}%")
    return total_loss / len(loader), acc

# --------------------------- Plots --------------------------
def save_model_summary(model: nn.Module, cfg: Config) -> None:
    cfg.save_dir.mkdir(parents=True, exist_ok=True)
    summary_str = str(summary(model, input_size=(cfg.batch_size, 1, cfg.img_size, cfg.img_size)))
    fig = plt.figure(figsize=(10, max(6, len(summary_str.splitlines()) * 0.22)))
    FigureCanvas(fig)
    fig.text(0, 1, summary_str, va="top", family="monospace", fontsize=6)
    fig.tight_layout()
    out_path = cfg.save_dir / "a_summary.png"
    fig.savefig(out_path, dpi=300)
    plt.close(fig)
    print("ğŸ–¼ (a) summary â†’", out_path)

def plot_history(train_loss: List[float], test_loss: List[float],
                 test_acc: List[float], cfg: Config) -> None:
    epochs_arr = np.arange(1, cfg.epochs + 1)
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_arr, train_loss, label="Train Loss")
    plt.plot(epochs_arr, test_loss,  label="Test Loss")
    plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Loss Curve"); plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(epochs_arr, test_acc,   label="Test Acc (%)")
    plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.title("Test Accuracy"); plt.legend()
    plt.tight_layout()
    out_path = cfg.save_dir / "b_history.png"
    plt.savefig(out_path, dpi=300)
    plt.close()
    print("ğŸ–¼ (b) history â†’", out_path)

@torch.no_grad()
def save_confusion(model: nn.Module, loader: DataLoader, classes: List[str],
                   cfg: Config) -> None:
    y_true, y_pred = [], []
    model.eval()
    for x, y in loader:
        logits = model(x.to(device))
        y_true.extend(y.cpu().numpy())
        y_pred.extend(logits.argmax(1).cpu().numpy())
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=classes, yticklabels=classes)
    plt.xlabel("Pred"); plt.ylabel("True"); plt.title("Confusion Matrix")
    plt.tight_layout()
    out_path = cfg.save_dir / "c_confmat.png"
    plt.savefig(out_path, dpi=300)
    plt.close()
    print("ğŸ–¼ (c) confusion â†’", out_path)

@torch.no_grad()
def save_examples(model: nn.Module, loader: DataLoader, classes: List[str],
                  k: int, cfg: Config) -> None:
    """Save k examples per class with predictions colored."""
    bucket: Dict[int, List[Tuple[torch.Tensor, bool, int]]] = {c: [] for c in range(len(classes))}
    model.eval()
    for x, y in loader:
        preds = model(x.to(device)).argmax(1).cpu()
        for img, true_label, pred in zip(x, y, preds):
            if len(bucket[true_label.item()]) < k:
                bucket[true_label.item()].append((img, pred.item() == true_label.item(), pred.item()))
        if all(len(v) == k for v in bucket.values()):
            break

    rows, cols = k, len(classes)
    plt.figure(figsize=(2.2 * cols, 2.2 * rows))
    for c in range(cols):
        for r in range(rows):
            img, ok, pred = bucket[c][r]
            ax = plt.subplot(rows, cols, r * cols + c + 1)
            ax.imshow(img.squeeze() * 0.5 + 0.5, cmap="gray")
            ax.set_title(f"T:{classes[c]}\nP:{classes[pred]}",
                         color="g" if ok else "r", fontsize=7)
            ax.axis("off")
    plt.suptitle("Test Predictions (green = âœ“, red = âœ—)")
    plt.tight_layout()
    out_path = cfg.save_dir / "d_examples.png"
    plt.savefig(out_path, dpi=300)
    plt.close()
    print("ğŸ–¼ (d) examples â†’", out_path)

# ------------------------------ Main ------------------------
def main(cfg: Config) -> None:
    print("===== Config =====")
    print(asdict(cfg))
    set_seed(cfg.seed)

    # 1. è³‡æ–™å‰è™•ç†
    unzip_dataset(cfg)
    train_test_split(cfg)
    train_loader, test_loader, classes = build_dataloaders(cfg)

    # 2. å»ºç«‹æ¨¡å‹
    model = ViT(
        image_size=cfg.img_size, patch_size=cfg.patch,
        num_classes=len(classes), dim=cfg.dim, depth=cfg.depth,
        heads=cfg.heads, mlp_dim=cfg.mlp_dim, channels=1
    ).to(device)
    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)
    criterion = nn.CrossEntropyLoss()

    # 3. æ¨¡å‹æ‘˜è¦ï¼ˆåœ– a éœ€è¦ï¼‰
    save_model_summary(model, cfg)

    # 4. è¨“ç·´
    train_hist, test_loss_hist, test_acc_hist = [], [], []
    start_time = time.time()
    for ep in range(1, cfg.epochs + 1):
        print(f"Epoch {ep}/{cfg.epochs}")
        tr_loss = train_one_epoch(model, train_loader, optimizer, criterion)
        te_loss, te_acc = evaluate(model, test_loader, criterion)

        train_hist.append(tr_loss)
        test_loss_hist.append(te_loss)
        test_acc_hist.append(te_acc)

    print(f"â± Total time {time.time() - start_time:.1f}s")

    # 5. å„²å­˜æ¨¡å‹æ¬Šé‡
    weight_path = f"{cfg.student_id}_Lab8_Homework.pth"
    torch.save(model.state_dict(), weight_path)
    print("âœ… Weights saved as", weight_path)

    # 6. ç¹ªåœ–
    cfg.save_dir.mkdir(parents=True, exist_ok=True)
    plot_history(train_hist, test_loss_hist, test_acc_hist, cfg)
    save_confusion(model, test_loader, classes, cfg)
    save_examples(model, test_loader, classes, k=4, cfg=cfg)

    print("\nğŸ‰ å››å¼µåœ–å…¨éƒ¨å®Œæˆï¼æª”æ¡ˆéƒ½åœ¨:", cfg.save_dir)

# --------------------- Script entry  ------------------------
if __name__ == "__main__":
    main(cfg)